{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a \"next word predictor\" using Text messages\n",
    "\n",
    "2/18/17 - Z. W. Miller\n",
    "\n",
    "We'll be using this dataset to build from: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:25:06.578894Z",
     "start_time": "2018-02-18T19:25:06.051052Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:25:06.865223Z",
     "start_time": "2018-02-18T19:25:06.583939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.2 |Anaconda custom (64-bit)| (default, Sep 21 2017, 18:29:43) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] \n",
      "\n",
      "Matplotlib Version: 2.0.2\n",
      "Numpy Version: 1.12.1\n",
      "Pandas Version: 0.20.3\n",
      "Scipy Version: 0.19.1\n",
      "Sklearn Version: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import matplotlib\n",
    "import sys\n",
    "import scipy\n",
    "\n",
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('Scipy', scipy), ('Sklearn', sklearn))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:36:50.157496Z",
     "start_time": "2018-02-18T19:36:49.946299Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "class next_word_predictor:\n",
    "    \n",
    "    def __init__(self, text, from_file=True, ngram=2):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.ngram = int(ngram)\n",
    "        self.keys = dict()\n",
    "        self._from_file = from_file\n",
    "        if type(text) != type(\"string\"):\n",
    "            raise TypeError(\"'text' must be a PATH or string object\")\n",
    "        if from_file:\n",
    "            self.path = text\n",
    "        else:\n",
    "            self.raw = text\n",
    "        self.text_as_list = None\n",
    "        self.create_probability_object()\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Opens and cleans the text to be learned from. If self.from_file, it reads\n",
    "        from the path provided. The cleaning is very minor, just lowercasing\n",
    "        and getting rid of quotes. Creates a list of words from the text.\n",
    "        \"\"\"\n",
    "        if self._from_file:\n",
    "            with open(self.path,'r') as f:\n",
    "                self.raw = f.read()\n",
    "        self.text_as_list = self.clean(self.raw)\n",
    "        \n",
    "    def group_generator(self,text_as_list):\n",
    "        \"\"\"\n",
    "        Generator that creates the ngram groupings to act as keys.\n",
    "        Just grabs ngram number of words and puts them into a tuple\n",
    "        and yields that upon iteration request.\n",
    "        ---\n",
    "        Inputs\n",
    "            text_as_list: the text after preprocessing (list)\n",
    "        Outputs\n",
    "            keys: word groupings of length self.ngram (tuple)\n",
    "        \"\"\"\n",
    "        if len(text_as_list) < self.ngram+1:\n",
    "            raise ValueError(\"NOT A LONG ENOUGH TEXT!\")\n",
    "            return\n",
    "\n",
    "        for i in range(self.ngram,len(text_as_list)):\n",
    "            yield tuple(text_as_list[i-self.ngram:i+1])\n",
    "\n",
    "    def create_probability_object(self):\n",
    "        \"\"\"\n",
    "        Steps through the text, pulling keys out and keeping track\n",
    "        of which words follow the keys. Duplication is allowed for \n",
    "        values for each key - but all keys are unique.\n",
    "        \"\"\"\n",
    "        if not self.text_as_list:\n",
    "            self.preprocess()\n",
    "        for group in self.group_generator(self.text_as_list):\n",
    "            word_key = tuple(group[:-1])\n",
    "            if word_key in self.keys:\n",
    "                if group[-1] in self.keys[word_key].keys():\n",
    "                    self.keys[word_key][group[-1]] += 1\n",
    "                else:\n",
    "                    self.keys[word_key][group[-1]] = 1\n",
    "            else:\n",
    "                self.keys[word_key] = {group[-1]: 1}\n",
    "                \n",
    "        self.common_words = Counter(self.text_as_list).most_common(3)\n",
    "        self.common_words, _ = zip(*self.common_words)\n",
    "        self.common_words = list(self.common_words)\n",
    "        \n",
    "    def clean(self, txt):\n",
    "        return (txt.lower().replace('\"','').replace(\"'\",\"\").replace(\"/\",\"\")\n",
    "                .replace(\".\",\"\").replace(\"!\",\"\").replace(\",\",\"\").replace(\"?\",\"\").split())\n",
    "    \n",
    "    def get_next_word_recs(self, current_text):\n",
    "        assert len(current_text.split()) >= self.ngram, \"Not enough words in input text!\"\n",
    "        \n",
    "        to_predict_from = self.clean(current_text)[-self.ngram:]\n",
    "        key_search = tuple(to_predict_from)\n",
    "        if key_search in self.keys:\n",
    "            top_3 = sorted(self.keys[key_search].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            rec_words, _ = zip(*top_3)\n",
    "            rec_words = list(rec_words)\n",
    "            words_to_add = 3 - len(rec_words)\n",
    "            rec_words = rec_words + self.common_words[:words_to_add]\n",
    "            return(rec_words)\n",
    "        else:\n",
    "            return self.common_words\n",
    "        \n",
    "    def print_key_value_pairs(self, num_keys=20):\n",
    "        \"\"\"\n",
    "        Iterates through the probability object, printing key-value\n",
    "        pairs. \n",
    "        ---\n",
    "        Input\n",
    "        num_keys: how many pairs to show (int)\n",
    "        \"\"\"\n",
    "        i = 1\n",
    "        for key,value in self.keys.items():\n",
    "            print(key,value)\n",
    "            print()\n",
    "            i+=1\n",
    "            if i>int(num_keys):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from the text messages in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:36:51.491232Z",
     "start_time": "2018-02-18T19:36:51.318067Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./smsspamcollection/SMSSpamCollection',sep='\\t', header=None)\n",
    "text = \"\"\n",
    "for line in df[1]:\n",
    "    text += line + \" \"\n",
    "    \n",
    "nwp = next_word_predictor(text[:-1], from_file=False, ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:13:58.819775Z",
     "start_time": "2018-02-18T19:13:58.720077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('go', 'until') {'jurong': 1}\n",
      "\n",
      "('until', 'jurong') {'point,': 1}\n",
      "\n",
      "('jurong', 'point,') {'crazy..': 1}\n",
      "\n",
      "('point,', 'crazy..') {'available': 1}\n",
      "\n",
      "('crazy..', 'available') {'only': 1}\n",
      "\n",
      "('available', 'only') {'in': 1}\n",
      "\n",
      "('only', 'in') {'bugis': 1, 'their': 1}\n",
      "\n",
      "('in', 'bugis') {'n': 1}\n",
      "\n",
      "('bugis', 'n') {'great': 1}\n",
      "\n",
      "('n', 'great') {'world': 1}\n",
      "\n",
      "('great', 'world') {'la': 1}\n",
      "\n",
      "('world', 'la') {'e': 1}\n",
      "\n",
      "('la', 'e') {'buffet...': 1}\n",
      "\n",
      "('e', 'buffet...') {'cine': 1}\n",
      "\n",
      "('buffet...', 'cine') {'there': 1}\n",
      "\n",
      "('cine', 'there') {'got': 1}\n",
      "\n",
      "('there', 'got') {'amore': 1}\n",
      "\n",
      "('got', 'amore') {'wat...': 1}\n",
      "\n",
      "('amore', 'wat...') {'ok': 1}\n",
      "\n",
      "('wat...', 'ok') {'lar...': 1}\n",
      "\n",
      "('ok', 'lar...') {'joking': 1}\n",
      "\n",
      "('lar...', 'joking') {'wif': 1}\n",
      "\n",
      "('joking', 'wif') {'u': 1}\n",
      "\n",
      "('wif', 'u') {'oni...': 1}\n",
      "\n",
      "('u', 'oni...') {'free': 1}\n",
      "\n",
      "('oni...', 'free') {'entry': 1}\n",
      "\n",
      "('free', 'entry') {'in': 4, '2': 7, 'into': 3, 'to': 1}\n",
      "\n",
      "('entry', 'in') {'2': 5}\n",
      "\n",
      "('in', '2') {'a': 5, 'or': 1}\n",
      "\n",
      "('2', 'a') {'wkly': 2, 'flyng': 1, 'weekly': 3, 'marriage.': 3}\n",
      "\n",
      "('a', 'wkly') {'comp': 2}\n",
      "\n",
      "('wkly', 'comp') {'to': 2, '4': 1}\n",
      "\n",
      "('comp', 'to') {'win': 2}\n",
      "\n",
      "('to', 'win') {'fa': 2, 'cash!': 2, 'a': 7, 'an': 3, '£100': 2, '£250': 1, 'the': 1, 'some': 1}\n",
      "\n",
      "('win', 'fa') {'cup': 2}\n",
      "\n",
      "('fa', 'cup') {'final': 2}\n",
      "\n",
      "('cup', 'final') {'tkts': 2, 'or': 2}\n",
      "\n",
      "('final', 'tkts') {'21st': 2}\n",
      "\n",
      "('tkts', '21st') {'may': 2}\n",
      "\n",
      "('21st', 'may') {'2005.': 2}\n",
      "\n",
      "('may', '2005.') {'text': 2}\n",
      "\n",
      "('2005.', 'text') {'fa': 2}\n",
      "\n",
      "('text', 'fa') {'to': 2}\n",
      "\n",
      "('fa', 'to') {'87121': 2}\n",
      "\n",
      "('to', '87121') {'to': 2, '18+6*£1.50(morefrmmob.': 2}\n",
      "\n",
      "('87121', 'to') {'receive': 2}\n",
      "\n",
      "('to', 'receive') {'entry': 2, 'a': 13, 'your': 2, 'the': 1, '£5000.00': 1, '£1000': 2, 'as': 2}\n",
      "\n",
      "('receive', 'entry') {'question(std': 2}\n",
      "\n",
      "('entry', 'question(std') {'txt': 2}\n",
      "\n",
      "('question(std', 'txt') {'rate)t&cs': 2}\n",
      "\n",
      "('txt', 'rate)t&cs') {'apply': 2}\n",
      "\n",
      "('rate)t&cs', 'apply') {'08452810075over18s': 2}\n",
      "\n",
      "('apply', '08452810075over18s') {'u': 1, 'new': 1}\n",
      "\n",
      "('08452810075over18s', 'u') {'dun': 1}\n",
      "\n",
      "('u', 'dun') {'say': 1, 'like': 3, 'haf': 1, 'wan': 5, 'drive': 1, 'have': 1}\n",
      "\n",
      "('dun', 'say') {'so': 1, 'already...': 1}\n",
      "\n",
      "('say', 'so') {'early': 1}\n",
      "\n",
      "('so', 'early') {'hor...': 1, 'cos': 1}\n",
      "\n",
      "('early', 'hor...') {'u': 1}\n",
      "\n",
      "('hor...', 'u') {'c': 1}\n",
      "\n",
      "('u', 'c') {'already': 1, 'miracle': 4, 'me': 1, 'd': 1, 'tiz': 1}\n",
      "\n",
      "('c', 'already') {'then': 1}\n",
      "\n",
      "('already', 'then') {'say...': 1}\n",
      "\n",
      "('then', 'say...') {'nah': 1}\n",
      "\n",
      "('say...', 'nah') {'i': 1}\n",
      "\n",
      "('nah', 'i') {'dont': 1}\n",
      "\n",
      "('i', 'dont') {'think': 7, 'want': 9, 'knw': 2, 'wanna': 1, 'live': 1, 'know': 21, 'have': 13, 'quite': 1, 'know,': 1, 'really': 2, 'see': 1, 'mind': 1, 'mind,i': 1, 'run': 2, 'meet': 1, 'care!': 1, 'go': 1, 'thnk': 3, 'get': 1, 'understand': 1, 'mean': 1, 'die': 1, 'let': 1, 'break': 1, 'care': 1, 'like': 2, 'make': 1, 'plan': 1, 'well': 1, 'tell': 1}\n",
      "\n",
      "('dont', 'think') {'he': 2, 'i': 3, 'so.': 1, 'doug': 1, 'ill': 1, 'so:)': 1, 'its': 1, 'about': 1, 'at': 1, 'you': 1}\n",
      "\n",
      "('think', 'he') {'goes': 1, 'has': 1, 'is': 1}\n",
      "\n",
      "('he', 'goes') {'to': 1}\n",
      "\n",
      "('goes', 'to') {'usf,': 1, 'work.': 1}\n",
      "\n",
      "('to', 'usf,') {'he': 1}\n",
      "\n",
      "('usf,', 'he') {'lives': 1}\n",
      "\n",
      "('he', 'lives') {'around': 1, 'with': 1}\n",
      "\n",
      "('lives', 'around') {'here': 1}\n",
      "\n",
      "('around', 'here') {'though': 1, 'easy': 1}\n",
      "\n",
      "('here', 'though') {'freemsg': 1, 'yep,': 1}\n",
      "\n",
      "('though', 'freemsg') {'hey': 1}\n",
      "\n",
      "('freemsg', 'hey') {'there': 1, 'u,': 1}\n",
      "\n",
      "('hey', 'there') {'darling': 1, 'babe,': 1}\n",
      "\n",
      "('there', 'darling') {'its': 1}\n",
      "\n",
      "('darling', 'its') {'been': 1}\n",
      "\n",
      "('its', 'been') {'3': 1, 'tough': 1, 'a': 1, 'quite': 1, 'almost': 1, 'ages.': 1, 'longer': 1}\n",
      "\n",
      "('been', '3') {'weeks': 1}\n",
      "\n",
      "('3', 'weeks') {'now': 1}\n",
      "\n",
      "('weeks', 'now') {'and': 1}\n",
      "\n",
      "('now', 'and') {'no': 1, 'lets': 3, 'also': 1, 'tell': 4, 'all': 1, 'wish': 1}\n",
      "\n",
      "('and', 'no') {'word': 1}\n",
      "\n",
      "('no', 'word') {'back!': 1, 'from': 1}\n",
      "\n",
      "('word', 'back!') {'id': 1}\n",
      "\n",
      "('back!', 'id') {'like': 1}\n",
      "\n",
      "('id', 'like') {'some': 1, 'to': 2}\n",
      "\n",
      "('like', 'some') {'fun': 1}\n",
      "\n",
      "('some', 'fun') {'you': 1, 'so': 1}\n",
      "\n",
      "('fun', 'you') {'up': 1}\n",
      "\n",
      "('you', 'up') {'for': 2, 'when': 1, 'bout': 2, 'to': 5, 'at': 2, 'there': 1, 'on': 1, 'as': 1, 'to.': 1}\n",
      "\n",
      "('up', 'for') {'it': 1, 'night': 1, 'http://www.bubbletext.com': 1, 'friday': 1, 'sale': 1, 'a': 2, 'us': 1, 'some': 2, 'the': 1, 'town': 1, 'like': 1, 'it.': 1, 'doin': 1}\n",
      "\n",
      "('for', 'it') {'still?': 1, 'on': 1, 'this': 1, '...': 1, 'like': 1, 'how': 1}\n",
      "\n",
      "('it', 'still?') {'tb': 1}\n",
      "\n",
      "('still?', 'tb') {'ok!': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nwp.print_key_value_pairs(num_keys=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T19:39:21.511013Z",
     "start_time": "2018-02-18T19:39:21.504977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell', 'lets', 'no']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwp.get_next_word_recs(\"now and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
