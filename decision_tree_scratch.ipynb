{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Decision Tree From Scratch (ZWM - 03/21/17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports (All imports here for cleanliness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Silly function to generate the test data, just to keep it clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Get some data, either pretend user data or the Iris dataset\n",
    "    ------\n",
    "    \"\"\"\n",
    "    # An oversimplified dataset for easy visualization while learning\n",
    "    \"\"\"headers = ['referral','country','faq','age','service']\n",
    "    my_data=[['slashdot','USA','yes',18,'None'],\n",
    "        ['google','France','yes',17,'Premium'],\n",
    "        ['google','France','yes',20,'Premium'],\n",
    "        ['reddit','USA','yes',24,'Basic'],\n",
    "        ['kiwitobes','France','yes',23,'Basic'],\n",
    "        ['google','UK','no',21,'Premium'],\n",
    "        ['(direct)','New Zealand','no',12,'None'],\n",
    "        ['(direct)','UK','no',21,'Basic'],\n",
    "        ['google','USA','no',24,'Premium'],\n",
    "        ['slashdot','France','yes',19,'None'],\n",
    "        ['slashdot','UK','yes',31,'Basic'],\n",
    "        ['reddit','USA','no',18,'None'],\n",
    "        ['google','UK','no',18,'None'],\n",
    "        ['kiwitobes','UK','no',19,'None'],\n",
    "        ['reddit','New Zealand','yes',12,'Basic'],\n",
    "        ['slashdot','UK','no',21,'None'],\n",
    "        ['google','UK','yes',18,'Basic'],\n",
    "        ['kiwitobes','France','yes',19,'Basic']]\n",
    "        \"\"\"\n",
    "    # The Iris data set from SKLearn \n",
    "    # (http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()\n",
    "    my_data = []\n",
    "    for x,y in zip(iris.data.tolist(),iris.target.tolist()):\n",
    "        my_data.append(x)\n",
    "        my_data[len(my_data)-1].append(y)\n",
    "    return my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tree building helper functions (the real mechanics between node finding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(data,colnum,value):\n",
    "    \"\"\"\n",
    "    Returns: Two sets of data from the initial data. Set 1 contains those that passed\n",
    "    the condition of data[colnum] >= value\n",
    "    ----------\n",
    "    Input: The dataset, the column to split on, the value on which to split\n",
    "    \"\"\"\n",
    "    splitter = None\n",
    "    if isinstance(value, int) or isinstance(value,float):\n",
    "        splitter = lambda x: x[colnum] >= value\n",
    "    else:\n",
    "        splitter = lambda x: x[colnum] == value\n",
    "    set1 = [row for row in data if splitter(row)]\n",
    "    set2 = [row for row in data if not splitter(row)]\n",
    "    return set1,set2\n",
    "\n",
    "def count_target_values(data):\n",
    "    \"\"\"\n",
    "    Returns: A dictionary of target variable counts in the data\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for row in data:\n",
    "        if row[-1] not in results:\n",
    "            results[row[-1]] = 0\n",
    "        results[row[-1]] += 1\n",
    "    return results\n",
    "    \n",
    "def entropy(data):\n",
    "    \"\"\"\n",
    "    Returns: Entropy of the data set, based on target values. \n",
    "    ent = Sum(-p_i Log(p_i), i in unique targets) where p is the percentage of the\n",
    "    data with the ith label.\n",
    "    Sidenote: We're using entropy as our measure of good splits. It corresponds to \n",
    "    information gained by making this split. If the split results in only one target type\n",
    "    then the entropy new sets entropy is 0. If it results in a ton of different targets, the\n",
    "    entropy will be high. \n",
    "    \"\"\"\n",
    "    results = count_target_values(data)\n",
    "    log2=lambda x:math.log(x)/math.log(2)\n",
    "    ent=0.\n",
    "    for r in results.keys():\n",
    "        p=float(results[r])/len(data) \n",
    "        ent-=p*log2(p)\n",
    "    return ent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The actual tree maker with node class for nesting (also has prediction and tree printer functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tree_node:\n",
    "    def __init__(self,col=-1,value=None,results=None,label=None,tb=None,fb=None):\n",
    "        self.col=col # column index of criteria being tested\n",
    "        self.value=value # vlaue necessary to get a true result\n",
    "        self.results=results # dict of results for a branch, None for everything except endpoints\n",
    "        self.tb=tb # true decision nodes \n",
    "        self.fb=fb # false decision nodes\n",
    "\n",
    "def train(data, scorefun=entropy, use_features=None):\n",
    "    if len(data) == 0: return tree_node()\n",
    "    current_score = scorefun(data)\n",
    "\n",
    "    best_gain = 0.0\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    if use_features==None:\n",
    "        use_features = [i for i in range(len(data[0]) - 1)]\n",
    "    for col in use_features:\n",
    "        # find different values in this column\n",
    "        column_values = set([row[col] for row in data])\n",
    "\n",
    "        # for each possible value, try to divide on that value\n",
    "        for value in column_values:\n",
    "            set1, set2 = split_data(data, col, value)\n",
    "\n",
    "            # Information gain\n",
    "            p = float(len(set1)) / len(data)\n",
    "            gain = current_score - p*scorefun(set1) - (1-p)*scorefun(set2)\n",
    "            if gain > best_gain and len(set1) > 0 and len(set2) > 0:\n",
    "                best_gain = gain\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "\n",
    "    if best_gain > 0:\n",
    "        trueBranch = train(best_sets[0],use_features=use_features)\n",
    "        falseBranch = train(best_sets[1],use_features=use_features)\n",
    "        return tree_node(col=best_criteria[0], value=best_criteria[1],\n",
    "                tb=trueBranch, fb=falseBranch)\n",
    "    else:\n",
    "        return tree_node(results=count_target_values(data))\n",
    "\n",
    "def print_tree(tree,indent=''):\n",
    "    if tree.results!=None: # if this is a end node\n",
    "        print str(tree.results)\n",
    "    else:\n",
    "        print 'Column ' + str(tree.col)+' : '+str(tree.value)+'? '\n",
    "        # Print the branches\n",
    "        print indent+' True: ',\n",
    "        print_tree(tree.tb,indent+indent)\n",
    "        print indent+' False: ',\n",
    "        print_tree(tree.fb,indent+indent)\n",
    "\n",
    "def predict(newdata, tree):\n",
    "    if tree.results!=None: # if this is a end node\n",
    "        return tree.results.keys()[0]\n",
    "    \n",
    "    if isinstance(newdata[tree.col], int) or isinstance(newdata[tree.col],float):\n",
    "        if newdata[tree.col] >= tree.value:\n",
    "            return predict(newdata, tree.tb)\n",
    "            \n",
    "        else:\n",
    "            return predict(newdata, tree.fb)\n",
    "    else:\n",
    "        if newdata[tree.col] == tree.value:\n",
    "            return predict(newdata, tree.tb)\n",
    "        else:\n",
    "            return predict(newdata, tree.fb) \n",
    "        \n",
    "def scorer(results):\n",
    "    correct = 0\n",
    "    for i in results:\n",
    "        if i[0] == i[1]:\n",
    "            correct+=1\n",
    "    return float(correct)/float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually get the data, train, test, and report accuracy (with bagging to see accuracy of our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data()\n",
    "\n",
    "\"\"\"\n",
    "--- Set Options ---\n",
    "random_forest chooses whether to use Random Forest Classifier mode.\n",
    "Random Forest Classifier has bagging and multiple trees generated\n",
    "with randomly selected features (sqrt(numFeatures) per tree). \n",
    "\n",
    "Not random_forest creates a single decision tree with/without bagging\n",
    "depending on if bagging == True.\n",
    "\"\"\"\n",
    "random_forest = True\n",
    "bagging = True\n",
    "num_trees = 100\n",
    "bagging_value = 100\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Bagged Result (Mode): 0.931818181818\n",
      "Average Accuracy for Single Tree: 0.8925\n"
     ]
    }
   ],
   "source": [
    "#Shuffle and split the data (shuffle because it comes sequentially by class)\n",
    "random.shuffle(data)\n",
    "train_split_index = int(0.7*len(data))\n",
    "train_data = data[0:train_split_index]\n",
    "test_data = data[train_split_index+1:]\n",
    "accuracy_values = []\n",
    "result_dict = {}\n",
    "\n",
    "if not random_forest:\n",
    "    num_trees = 1\n",
    "if not bagging:\n",
    "    bagging_value = 1\n",
    "    \n",
    "for _ in range(num_trees):\n",
    "    valid_cols = []\n",
    "    if random_forest:\n",
    "        num_cols_select = int(math.sqrt(len(data[0]))+0.5)\n",
    "        possible_cols = [i for i in range(len(data[0]) - 1)]\n",
    "        for _ in range(num_cols_select):\n",
    "            random.shuffle(possible_cols)\n",
    "            valid_cols.append(possible_cols.pop())\n",
    "    else:\n",
    "        valid_cols = [i for i in range(len(data[0]) - 1)]\n",
    "    \n",
    "    bagged_data = [random.choice(train_data) for _ in range(bagging_value)]\n",
    "    tree = train(bagged_data,use_features=valid_cols)\n",
    "    # print_tree(tree,'---') # For visualizing the tree splits\n",
    "\n",
    "    results = []\n",
    "    for i,new_data in enumerate(test_data):\n",
    "        if i not in result_dict.keys(): \n",
    "            result_dict[i] = []\n",
    "        ypred = predict(new_data, tree)\n",
    "        y = new_data[-1]\n",
    "        results.append([y,ypred])\n",
    "        result_dict[i].append(ypred)\n",
    "    accuracy_values.append(scorer(results))\n",
    "\n",
    "from collections import Counter\n",
    "bag_result_mode = []\n",
    "for i,data in enumerate(test_data):\n",
    "    bag_result_mode.append([Counter(result_dict[i]).most_common(1)[0][0],test_data[i][-1]])\n",
    "    \n",
    "print \"Accuracy for Bagged Result (Mode): \" + str(scorer(bag_result_mode))  \n",
    "print \"Average Accuracy for Single Tree: \" + str(sum(accuracy_values)/len(accuracy_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with SkLearn Classifiers (\"professional\" trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree from SkLearn: 0.977777777778\n",
      "Accuracy of Random Forest from SkLearn: 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "X, Y = load_iris().data, load_iris().target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.3)\n",
    "\n",
    "sktree = DecisionTreeClassifier(criterion='entropy')\n",
    "skrf = RandomForestClassifier(n_estimators = 100,criterion='entropy')\n",
    "\n",
    "sktree.fit(x_train,y_train)\n",
    "skrf.fit(x_train,y_train)\n",
    "\n",
    "print \"Accuracy of Decision Tree from SkLearn: \" + str(sktree.score(x_test,y_test))\n",
    "print \"Accuracy of Random Forest from SkLearn: \" + str(skrf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
